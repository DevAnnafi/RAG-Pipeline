"""
Complete Document Intelligence Pipeline with Gradio UI
Combines DocumentProcessor, RAGPipeline, and interactive web interface
"""

# ============================================================================
# SETUP & DEPENDENCIES
# ============================================================================

import subprocess
import sys

print("Installing dependencies...")
packages = [
    'gradio', 'sentence-transformers', 'faiss-cpu', 'PyPDF2',
    'pillow', 'pytesseract', 'python-dotenv', 'numpy', 'scikit-learn'
]

for package in packages:
    try:
        __import__(package.replace('-', '_'))
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

# System dependencies for OCR
try:
    import pytesseract
    pytesseract.pytesseract.pytesseract_cmd  # Test if available
except:
    subprocess.run(["apt-get", "install", "-y", "tesseract-ocr"],
                   capture_output=True)

import os
import re
import json
import hashlib
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Tuple, List, Dict, Optional

import numpy as np
import PyPDF2
from PIL import Image
import pytesseract
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr

print("‚úÖ All dependencies installed and imported!\n")

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Centralized configuration for RAG pipeline"""
    CHUNK_SIZE = 800          # Characters per chunk
    CHUNK_OVERLAP = 150       # Overlap between chunks
    MIN_CONTENT_LENGTH = 20   # Minimum content length
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
    EMBEDDING_DIM = 384
    DEVICE = "cpu"
    TOP_K = 5                 # Top K chunks to retrieve
    SIMILARITY_THRESHOLD = 0.3
    LLM_TEMPERATURE = 0.7
    LLM_MAX_TOKENS = 500

config = Config()

# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class DocumentMetadata:
    """Metadata about processed documents"""
    doc_id: str
    filename: str
    doc_type: str
    page_count: int
    upload_date: str
    source_id: str
    chunk_count: int
    file_size: int = 0

@dataclass
class TextChunk:
    """Represents a single text chunk"""
    chunk_id: str
    doc_id: str
    content: str
    page_num: int
    chunk_index: int
    metadata: Dict

@dataclass
class RetrievalResult:
    """Retrieval result with similarity score"""
    chunk: TextChunk
    similarity_score: float
    document: DocumentMetadata

# ============================================================================
# DOCUMENT PROCESSOR
# ============================================================================

class DocumentProcessor:
    """Process documents and create semantic chunks"""

    def __init__(self):
        self.document_store = {}  # Store document metadata
        self.chunks_store = {}    # Store chunks for reference
        self.chunk_counter = 0    # Global chunk counter

    def process_document(self, file_path: str, doc_type: str = "Document") -> Tuple[DocumentMetadata, List[TextChunk]]:
        """
        Main document processing pipeline:
        1. Read file (handle multiple formats)
        2. Extract text (with OCR for images)
        3. Clean text
        4. Create overlapping chunks
        5. Generate metadata
        """

        doc_id = f"doc_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}"
        filename = Path(file_path).name
        file_size = os.path.getsize(file_path)

        print(f"\nüìÑ Processing: {filename}")
        print(f"   Type: {doc_type}")

        # STEP 1: READ THE FILE
        text = ""
        page_count = 1

        try:
            if str(file_path).endswith(('.txt', '.text')):
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                print("   ‚úì Read text file")

            elif str(file_path).endswith('.pdf'):
                try:
                    with open(file_path, 'rb') as f:
                        pdf_reader = PyPDF2.PdfReader(f)
                        page_count = len(pdf_reader.pages)
                        for page in pdf_reader.pages:
                            text += page.extract_text() + "\n"
                    print(f"   ‚úì Extracted PDF ({page_count} pages)")
                except Exception as pdf_err:
                    print(f"   ‚ö†Ô∏è PDF error: {pdf_err}")
                    raise

            elif str(file_path).endswith(('.png', '.jpg', '.jpeg')):
                try:
                    image = Image.open(file_path)
                    text = pytesseract.image_to_string(image)
                    print("   ‚úì Extracted text from image (OCR)")
                except Exception as ocr_err:
                    print(f"   ‚ö†Ô∏è OCR error: {ocr_err}")
                    raise

            else:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()
                print("   ‚úì Read as text")

        except Exception as e:
            print(f"   ‚ùå Failed to read: {e}")
            raise ValueError(f"Cannot read {filename}")

        if not text or len(text.strip()) < config.MIN_CONTENT_LENGTH:
            raise ValueError(f"No content in {filename}")

        print(f"   ‚úì Extracted {len(text)} characters")

        # STEP 2: CLEAN TEXT
        text = re.sub(r'\s+', ' ', text).strip()
        print(f"   ‚úì Cleaned text")

        # STEP 3: CREATE OVERLAPPING CHUNKS
        chunks = []
        chunk_list = self._create_chunks(text, config.CHUNK_SIZE, config.CHUNK_OVERLAP)

        for i, chunk_text in enumerate(chunk_list):
            chunk_id = f"chunk_{self.chunk_counter}"
            page_num = (i * config.CHUNK_SIZE) // 2000 + 1

            chunk = TextChunk(
                chunk_id=chunk_id,
                doc_id=doc_id,
                content=chunk_text,
                page_num=min(page_num, page_count),
                chunk_index=i,
                metadata={
                    "doc_type": doc_type,
                    "char_count": len(chunk_text),
                    "source_id": doc_id
                }
            )
            chunks.append(chunk)
            self.chunks_store[chunk_id] = chunk
            self.chunk_counter += 1

        print(f"   ‚úì Created {len(chunks)} chunks")

        # STEP 4: CREATE METADATA
        metadata = DocumentMetadata(
            doc_id=doc_id,
            filename=filename,
            doc_type=doc_type,
            page_count=page_count,
            upload_date=datetime.now().isoformat(),
            source_id=doc_id,
            chunk_count=len(chunks),
            file_size=file_size
        )

        self.document_store[doc_id] = metadata
        return metadata, chunks

    def _create_chunks(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Split text into overlapping chunks"""
        chunks = []
        step = chunk_size - overlap

        for i in range(0, len(text), step):
            chunk = text[i:i + chunk_size]
            if chunk.strip():
                chunks.append(chunk)

        return chunks if chunks else [text]

# ============================================================================
# EMBEDDING MANAGER
# ============================================================================

class EmbeddingManager:
    """Manage embeddings and FAISS vector search"""

    def __init__(self, model_name: str = config.EMBEDDING_MODEL):
        print(f"üîÑ Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name, device=config.DEVICE)
        self.index = faiss.IndexFlatL2(config.EMBEDDING_DIM)
        self.chunk_map = {}
        self.embeddings = []
        print("‚úì Embedding model loaded\n")

    def add_chunks(self, chunks: List[TextChunk]) -> None:
        """Add chunks to FAISS index"""
        if not chunks:
            return

        print(f"üìç Indexing {len(chunks)} chunks...")

        texts = [chunk.content for chunk in chunks]
        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)

        for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):
            self.embeddings.append(emb)
            self.chunk_map[len(self.embeddings) - 1] = chunk

        embeddings_array = np.array(self.embeddings, dtype=np.float32)
        self.index = faiss.IndexFlatL2(config.EMBEDDING_DIM)
        self.index.add(embeddings_array)
        print(f"‚úì Added {len(chunks)} chunks to index\n")

    def retrieve(self, query: str, k: int = config.TOP_K) -> List[Dict]:
        """Retrieve top-k most similar chunks"""
        query_embedding = self.model.encode([query], convert_to_numpy=True, show_progress_bar=False)
        distances, indices = self.index.search(query_embedding, min(k, len(self.chunk_map)))

        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx in self.chunk_map:
                chunk = self.chunk_map[idx]
                similarity = 1 / (1 + distance)

                if similarity >= config.SIMILARITY_THRESHOLD:
                    results.append({
                        'chunk': chunk,
                        'similarity': float(similarity),
                        'distance': float(distance)
                    })

        return sorted(results, key=lambda x: x['similarity'], reverse=True)

# ============================================================================
# RESPONSE GENERATOR
# ============================================================================

class ResponseGenerator:
    """Generate answers from retrieved context"""

    def __init__(self):
        print("ü§ñ Initializing response generator...")
        self.initialized = True

    def generate_answer(self, query: str, context: List[Dict], num_chunks: int) -> Tuple[str, float]:
        """Generate answer from context"""

        if not context:
            return "No relevant documents found. Please upload documents first.", 0.0

        try:
            # ----------------------------------------------------------------
            # MULTI-DOC SYNTHESIS (Option C)
            # Choose top chunk per document (by similarity), then compose answer
            # across distinct documents instead of anchoring to context[0] only.
            # ----------------------------------------------------------------

            # 1) Pick best chunk per doc_id
            best_per_doc: Dict[str, Dict] = {}
            for r in context:
                chunk = r.get("chunk")
                if not chunk:
                    continue
                doc_id = getattr(chunk, "doc_id", None)
                if not doc_id:
                    continue

                if doc_id not in best_per_doc or r.get("similarity", 0.0) > best_per_doc[doc_id].get("similarity", 0.0):
                    best_per_doc[doc_id] = r

            # 2) Sort selected docs by similarity desc and cap to num_chunks
            selected = sorted(best_per_doc.values(), key=lambda x: x.get("similarity", 0.0), reverse=True)
            if num_chunks and num_chunks > 0:
                selected = selected[:num_chunks]

            # 3) Build answer by extracting relevant sentences from each doc‚Äôs top chunk
            query_words = set(query.lower().split())
            answer_parts: List[str] = []

            for r in selected:
                chunk = r["chunk"]
                text = chunk.content.strip()
                if not text:
                    continue

                # simple sentence split; stays consistent with your original approach
                sentences = text.split(". ")
                relevant_sentences = []
                for sent in sentences:
                    sent_clean = sent.strip()
                    if not sent_clean:
                        continue
                    sent_words = set(sent_clean.lower().split())
                    # prefer sentences that overlap query; otherwise take a couple early sentences
                    if len(query_words & sent_words) > 1:
                        relevant_sentences.append(sent_clean)
                    elif len(relevant_sentences) < 1:
                        relevant_sentences.append(sent_clean)

                    if len(relevant_sentences) >= 2:
                        break

                if relevant_sentences:
                    answer_parts.append(". ".join(relevant_sentences).rstrip(".") + ".")

            # 4) If nothing extracted, fallback to concatenated snippets from selected docs
            if not answer_parts:
                snippets = []
                for r in selected:
                    c = r["chunk"].content[:250].strip()
                    if c:
                        snippets.append(c)
                if snippets:
                    answer = " ".join(snippets)
                    if not answer.endswith((".", "?", "!")):
                        answer += "."
                else:
                    answer = "No relevant content found in the retrieved context."
            else:
                answer = " ".join(answer_parts).strip()

            # Confidence stays based on best match (same as your earlier behavior)
            confidence = context[0]["similarity"] if context else 0.0
            confidence = min(1.0, max(0.0, float(confidence)))

            return answer, confidence

        except Exception as e:
            print(f"‚ùå Error in answer generation: {e}")
            if context:
                fallback = context[0]['chunk'].content[:300]
                return f"Based on the documents: {fallback}...", 0.5
            return "Unable to generate answer.", 0.0

# ============================================================================
# RAG PIPELINE
# ============================================================================

class RAGPipeline:
    """Complete RAG pipeline combining all components"""

    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.embedding_manager = EmbeddingManager()
        self.response_generator = ResponseGenerator()
        self.documents = {}
        self.chat_history = []

    def add_document(self, file_path: str, doc_type: str = "Document") -> str:
        """Process and add document to pipeline"""
        try:
            metadata, chunks = self.document_processor.process_document(file_path, doc_type)
            self.documents[metadata.doc_id] = metadata
            self.embedding_manager.add_chunks(chunks)
            return f"‚úÖ Processed: {metadata.filename}\n   üìä Chunks: {len(chunks)}\n   üìÑ Pages: {metadata.page_count}"
        except Exception as e:
            return f"‚ùå Error: {str(e)}"

    def query(self, question: str) -> Tuple[str, List[Dict], float]:
        """Execute full RAG pipeline"""

        try:
            if not self.documents or len(self.embedding_manager.chunk_map) == 0:
                return "‚ùå No documents uploaded. Please upload files first.", [], 0.0

            retrieval_results = self.embedding_manager.retrieve(question, k=config.TOP_K)

            if not retrieval_results:
                return "‚ùå No relevant information found.", [], 0.0

            answer, confidence = self.response_generator.generate_answer(
                question, retrieval_results, len(retrieval_results)
            )

            sources = []
            for result in retrieval_results:
                try:
                    chunk = result['chunk']
                    doc = self.documents.get(chunk.doc_id, None)

                    sources.append({
                        'filename': doc.filename if doc else 'Unknown',
                        'page': chunk.page_num,
                        'similarity': result['similarity'],
                        'excerpt': chunk.content[:200].strip()
                    })
                except Exception as e:
                    continue

            self.chat_history.append({
                'question': question,
                'answer': answer,
                'sources': sources,
                'confidence': confidence,
                'timestamp': datetime.now().isoformat()
            })

            return answer, sources, confidence

        except Exception as e:
            print(f"‚ùå Query error: {str(e)}")
            return f"Error: {str(e)}", [], 0.0

# ============================================================================
# GRADIO INTERFACE
# ============================================================================

def build_gradio_ui(pipeline: RAGPipeline):
    """Build Gradio web interface"""

    def upload_document(files, doc_type):
        """Handle document upload"""
        if not files:
            return "No files selected"

        messages = []
        for file in files:
            msg = pipeline.add_document(file.name, doc_type)
            messages.append(msg)
        return "\n".join(messages)

    def chat(question):
        """Handle user question"""
        if not question.strip():
            return "Please enter a question", ""

        try:
            answer, sources, confidence = pipeline.query(question)

            sources_text = f"**Confidence: {confidence:.0%}** | **Sources: {len(sources)}**\n\n"

            if sources:
                sources_text += "**üìö Sources:**\n"
                for i, source in enumerate(sources, 1):
                    sim_score = min(1.0, source['similarity'])
                    sources_text += f"\n**{i}. {source['filename']}** (Page {source['page']}, Match: {sim_score:.0%})\n"
                    sources_text += f"> {source['excerpt']}\n"
            else:
                sources_text += "*No sources found*"

            return answer, sources_text
        except Exception as e:
            return f"Error: {str(e)}", f"Error details: {str(e)}"

    # Build Gradio Blocks interface
    with gr.Blocks(title="Document RAG Pipeline", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# üìÑ Document Intelligence Pipeline")
        gr.Markdown("Upload documents and ask questions about their contents using RAG")

        # TAB 1: CHAT
        with gr.Tab("üí¨ Chat"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="Your Question",
                        placeholder="Ask about your documents...",
                        lines=2
                    )
                    ask_btn = gr.Button("Ask", scale=1, variant="primary")

            with gr.Row():
                answer_output = gr.Textbox(
                    label="üìù Answer",
                    interactive=False,
                    lines=6
                )
                sources_output = gr.Markdown(label="üìö Sources & Confidence")

            ask_btn.click(chat, inputs=question_input, outputs=[answer_output, sources_output])

        # TAB 2: UPLOAD DOCUMENTS
        with gr.Tab("üì§ Upload Documents"):
            gr.Markdown("### Upload and process your documents")
            file_input = gr.File(
                label="Upload Files (PDF, TXT, Images)",
                file_count="multiple",
                type="filepath"
            )
            doc_type = gr.Dropdown(
                choices=["Mortgage Agreement", "Financial Report", "Legal Document", "Technical Manual", "Other"],
                value="Document",
                label="Document Type"
            )
            upload_btn = gr.Button("Process Documents", variant="primary")
            upload_output = gr.Textbox(
                label="Upload Status",
                interactive=False,
                lines=5
            )

            upload_btn.click(upload_document, inputs=[file_input, doc_type], outputs=upload_output)

        # TAB 3: CHAT HISTORY
        with gr.Tab("üìú Chat History"):
            def update_history():
                if not pipeline.chat_history:
                    return "No chats yet"
                history_text = ""
                for i, chat in enumerate(pipeline.chat_history, 1):
                    history_text += f"\n{'='*50}\n"
                    history_text += f"**Q{i}:** {chat['question']}\n"
                    history_text += f"**A{i}:** {chat['answer']}\n"
                    history_text += f"**Confidence:** {chat['confidence']:.0%} | **Sources:** {len(chat['sources'])}\n"
                return history_text

            history_output = gr.Markdown(label="Chat History")
            refresh_btn = gr.Button("Refresh History")
            refresh_btn.click(update_history, outputs=history_output)

        # TAB 4: ABOUT
        with gr.Tab("‚ÑπÔ∏è About"):
            gr.Markdown("""
## Document Intelligence Pipeline

### What is this?
A Retrieval-Augmented Generation (RAG) system that:
- Processes documents (PDF, TXT, Images with OCR)
- Creates semantic chunks with overlaps
- Generates embeddings using Sentence-BERT
- Searches using FAISS vector database
- Retrieves relevant context and generates answers

### How it works:
1. **Upload** ‚Üí Documents are processed and chunked
2. **Embed** ‚Üí Text converted to 384-dim vectors
3. **Index** ‚Üí Vectors stored in FAISS
4. **Query** ‚Üí Question matched to relevant chunks
5. **Answer** ‚Üí Context extracted and formatted

### Features:
- ‚úÖ Multi-format support (PDF, TXT, PNG, JPG)
- ‚úÖ Automatic OCR for images
- ‚úÖ Semantic chunking with overlap
- ‚úÖ Fast FAISS similarity search
- ‚úÖ Confidence scoring
- ‚úÖ Chat history tracking
- ‚úÖ Source citation

### Performance:
- Embedding: Sentence-BERT all-MiniLM-L6-v2
- Vector DB: FAISS IndexFlatL2
- Chunk size: 800 characters with 150 overlap
- Top-K retrieval: 5 results
- Response generation: Extractive approach
            """)

    return interface

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("üöÄ DOCUMENT INTELLIGENCE PIPELINE - GRADIO VERSION")
    print("=" * 80)

    # Initialize pipeline
    pipeline = RAGPipeline()

    # Build and launch interface
    print("\nüìä Building Gradio interface...")
    interface = build_gradio_ui(pipeline)

    print("‚úÖ Launch complete!")
    print("   üåê Access at: http://localhost:7860")
    print("\nüìù Start by uploading documents, then ask questions!\n")

    interface.launch(share=True, show_error=True)

